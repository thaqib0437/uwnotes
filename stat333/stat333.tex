\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{lmodern}


\usepackage[
	name=Thaqib\ M, 
	class= stat333, 
	doctype= Notes, 
	boxed=false,] 
{template}
\renewcommand{\arraystretch}{3} % <--------------


\author{Thaqib M}
\title{stat333 Notes}

\begin{document}
\maketitle
\newpage
\section*{Linear Algebra}
\subsubsection*{Matrix multiplication} If $A$ is a $n\times m$ matrix and $B$ is a $m\times k$ matrix then the matrix $AB$ of $\dim$ $n\times k$ is defined by: 
\begin{align*}
{\bf[}AB{\bf]}_{xy} = \nsum{\all(z)}{}A_{xz}B_{zy}
\end{align*}
\\
\subsubsection*{Inner Product} The inner product (dot product) of 2 vectors $\vec{a}, \vec{b}$ in $\R^n$ is defined as
\begin{align*}
\va{a}\cdot \va{b} = \innerp{\va{a},\va{b}} = \nsum{k=1}{n}a_kb_k
\end{align*}
\subsubsection*{Eigenvalues and Eigenvectors}
We can find eigenvalues by solving for the roots of the characteristic polynomial of the matrix $\vb A$.
\begin{align*}
\det(\vb A-tI_n) = 0
\end{align*}
Where $I_n$ is the $n\times n$ identity matrix.
Then for each eigenvalue $t=c$ we can solve the system of linear equations 
\begin{align*}
(\vb A - cI_n)\va* x = \va* 0
\end{align*}
$\va* x$ will be an eigenvector of $\vb A$. 
\newpage
\section*{Assignment Theorems}
\begin{align*}
\E[Xf(Y)] = \E[f(Y)\cdot\E(X\given Y)] \tag{hw1q6}
\end{align*}
\begin{align*}
\E[X] = \nsum{k=1}{\infty}k\P(X=k) = \nsum{k=1}{\infty} \P(X\geq k) \tag{hw2q5}
\end{align*}
\begin{align*}
\{X_n\} \asto X \Rightarrow \{X_n\}\pto X \tag{hw3q5}
\end{align*}
\newpage
\section*{Stat}
\begin{align*}
\E[X] = \E[\E[X|Y]] \tag{tower rule}
\end{align*}
\newpage
\section{Week 1}
\begin{defn}[\textbf{Stochastic Process}]
Let $(X_t)_{t\in T}$ be a collection of random variables this is called a Stochastic Process. $T$ is the \textit{index set}. 
\end{defn}
\begin{example}[Simple Random Walk on $\Z$]
Let $X_i\sim \iid$ where $X_i \in \{-1,1\}$ with 
\begin{align*}
P(X_i = 1) = \frac{1}{2}\\
P(X_i = -1) = \frac{1}{2}
\end{align*}
now let 
\begin{align*}
S_n = \sum_{i=0}^n X_i
\end{align*}
Then $(S_i)_{k=0}^\infty$ is a stochastic process.  
\end{example}

\begin{defn}[\textbf{Transition Probability}]
Given $(X_s)_{s \leq t}$ we need the probability for $X_{t+1}$.
\begin{align*}
P(X_{(t+1)} = x_{t+1} \vert X_1 = x_1 , X_2 = x_2, \ldots X_t = x_t)
\end{align*}
\end{defn}


\begin{note}{Conditional Probability Properties}
\begin{align*}
&P(A | B) = \frac{P(AB)}{P(B)} \; P(B) > 0\\
&P(ABC) = P(A|BC) \cdot P(B|C) \cdot P(C)
\end{align*}
\end{note}

\begin{example}{Transition Probabilities for SRW on $\Z^d$}
\begin{align*}
P\left(\norm{X_{t+1} - X_{t}} \given (X_{s})_{s \leq t} \right) = \frac{1}{2d} 
\end{align*}
\end{example}
\newpage
\subsection{Markov Chains}
\begin{defn}[{Markov Property}]
\label{markovprop}
A process has the Markov property if:
\begin{align*}
P(X_{t+1}= x_{t+1} \given (X_{s})_{s\leq t}) = P(X_{t+1} = x_{t+1} \given X_{t} = x_t)
\end{align*}
(Next outcome only depends on the previous outcome)
\end{defn}

\begin{note}[Markov Chain]
A stochastic process that satisfies the \hyperref[markovprop]{Markov property} is called a Markov chain.
\end{note}

\begin{defn}[Time Homogeneous Markov Chain]
A Markov Chain is called time homogeneous if the following is true
\begin{align*}
P(X_{t+1} = j \given X_t = i) = P(X_1 = j \given X_0 = i)
\end{align*}
\end{defn}

\begin{defn}[Stochastic Matrix]
\label{stocmat}

A matrix $\mathbf{P}$ is called stochastic if
\begin{align*}
&\mathbf{P} = \mqty(
p_{00} & p_{01} & \ldots \\
p_{10} & p_{11} & \ldots \\
\vdots  & \ddots &
)\\
&0\leq p_{ij} \leq 1\\
&\nsum{all(j)}{} p_{i_0j} = 1 \; \text{ for fixed $i_0$}
\end{align*}
\end{defn}
\begin{defn}[Transition Matrix]
Let $\mathbf{P}$ be a \hyperref[stocmat]{Stochastic matrix} and let $p_{ij} = $ value in $i-$th row and $j-$th column.
We define $p_{ij}$ as
\begin{align*}
&p_{ij} = P(X_{t} = j \given X_{t-1} = i)\\
\end{align*}
(probability of going from state $i$ to state $j$ in the chain). \\
This is called the transition matrix for $(X_t)_{t \in T}$.
\end{defn}
\newpage
\begin{example}{Transition Matrix}
Consider this transition matrix 
The transition matrix for this Markov Chain is
\[
\begin{blockarray}{cccc}
& 1 & 2 & 3 \\
\begin{block}{c(ccc)}
1 & 0 & \frac{1}{2} & \frac{1}{2} \\
2 & \frac{1}{3} & 0 & \frac{2}{3} \\
3 & \frac{1}{3} & \frac{2}{3} & 0 \\
\end{block}
\end{blockarray}
\]
this can be visualized as: 
\fig{fig0}{0.5}
\end{example}
\newpage
\subsubsection{Multistep Transition Probabilities}
\begin{defn}
\begin{align*}
[P(n, n+m)]_{xy} = P(X_{n+m} = y \given X_{n} = x)
\end{align*}
\end{defn}

\begin{thm}{Multistep Transition Probability Matrix}
\label{thm113}
Let $(X_t)_{t\in T}$ be a stochastic process satisfying the Markov property and be \textit{time homogeneous} and let $\bf P$ be the transition matrix.  
\begin{align*}
[P(n, n+m)]_{xy} = \mathbf{P}^m_{xy}
\end{align*}
\end{thm}
\begin{lemma}{}
\label{lem114}
\begin{align*}
[P(n, m+1+n)]_{xy} = \nsum{\mathrm{all}(z)}{}[P(n,m+n)]_{xz}P_{zy}
\end{align*}
\end{lemma}
\begin{proof}
To go from state $x \to y$ we must add up all probabilities of going to an intermediate state $\bf z$, $x \to {\bf z} \to y$ we add possibilities of $\bf z$. 
\begin{align*}
&[P(n, m+1+n)]_{xy} = P(X_{m+1+n} = y \given X_{n} = x)\\
&= \nsum{\mathrm{all}(z)}{} P(X_{m+1+n} = y, X_{n+m} = z \given X_{n} = x) \text{ Marginal probability function (stat240) }\\
&= \nsum{\mathrm{all}(z)}{} P(X_{m+1+n} = y \given X_{n+m} = z , X_{n} = x)P(X_{n+m} = z \given X_n = x) \text{ conditional probability }
\end{align*}
Since $X_t$ satisfies the Markov property we get
\begin{align*}
&= \nsum{\mathrm{all}(z)}{} P(X_{m+1+n} = y \given X_{n+m} = z)P(X_{n+m} = z \given X_n = x)
\end{align*}
By definition we have $P(X_{m+1+n} = y \given X_{n+m} = z) = P_zy$ and $P(X_{n+m} = z \given X_n = x) = [P(n, n+m)]_{xz}$. 
\end{proof}
Using \hyperref[lem114]{Lemma 1.14} we can prove the \hyperref[thm113]{Theorem 1.13}. \\
Since  \hyperref[lem114]{1.14}'s result is the definition of matrix multiplication we get
\begin{align*}
[P(n, m+1+n)]_{xy} = [P(n, m+n)P]_{xy}
\end{align*}
by induction on $m$ with base case $P(n, n+1) = P$ we get
\begin{align*}
&[P(n, m+1+n)]_{xy} = {\bf P}^m
\end{align*}
Since RHS does not depend on $n$ we can write $P(n, n+m) = P(m)$ and time homogeneity applies for any $m$ number of steps. 
\begin{align*}
P(X_{n+m} = y \given X_n = x) = P(X_{m} = y \given X_0 = x)
\end{align*}  

\newpage 

\section{Week 2}
\subsection{Initial Data}
Let $(X_n)_{n\in I}$ be a time homogeneous Markov chain. We denote these by $0, 1, 2, \ldots \abs{I} - 1$. We represent the state space as:
\begin{align*}
\{i_1, i_2, \ldots, i_{\abs{I}}\} = \mathcal{X}
\end{align*}
Let $\bf P$ be the transition matrix for this Markov chain. 
\begin{defn}[Distribution Row Vector]
\begin{align*}
\mu_j = P(X_0 = i_j)
\end{align*}
Then the row vector $\vec{\mu}$ of $\dim = 1 \times \abs{I}$ is defined as 
\begin{align*}
\vec{\mu} = \left[\mu_1, \mu_2, \ldots, \mu_{\abs{I}}\right]
\end{align*} 
$\vec{\mu}$ is called the distribution of $X_0$ denoted by $X_0 \sim \vec{mu}$.\\
The distribution vector for $X_n$ is denoted by $\mu(n)$. 
\end{defn}


\begin{thm}{Distribution of $X_n$}
The distribution row vector of $X_n$ for a time homogeneous Markov chain is given by $\mu P^n$  
\end{thm}
\begin{proof}
\textit{Sketch}. 
\begin{align*}
P(X_n = i_k) = \nsum{j=1}{\abs{I}}P(X_n = i_k \given X_0 = i_j)P(X_0 = i_j) = \sum \vec{\mu_j} P_{jk} = \left[\vec{\mu}P\right]_k
\end{align*}
Implies $X_n \sim \vec{\mu}P^n$
\end{proof}
\newpage
\subsection{Conditional Expectation}
Given $f: \mathcal{X}\to \mathbb{R}$ what is the expected value of $f(X_m)$ given an initial distribution? 
\\
The function $f$ on a finite state space $\mathcal{X}$ is equivalent to a vector $\vec{f} \in \R^{\abs{\mathcal{X}}}$
\begin{align*}
\vec{f} = \mqty(f(1)\\f(2)\\ \ldots \\ f(n))
\end{align*}  
The conditional expectation for $f(X_m)$ given $X_0 \sim \vec{\mu}$ is denoted by 
\begin{align*}
\E(f(X_m) \given X_0 \sim \vec{\mu})
\end{align*} 
By definition of conditional expectation we get
\begin{align*}
\E(f(X_m) \given X_0 \sim \vec{\mu}) &= \nsum{k=1}{\abs{\mathcal{X}}} f(i_k)P(X_m = i_k \given X_0 \sim \vec{\mu})\\
&= \nsum{\all(k)}{} f(i_k) \left[\va*{\mu} \vb{P}^m \right]_k\\
&= \nsum{\all(k)}{} \va*{f}_k \left[\va*{\mu} \vb{P}^m \right]_k\\
&= \innerp{\va*{\mu}\vb{P}^m, \va*{f}}
\end{align*}
\newpage
\subsection{Stationary Distribution}
Suppose $X_0 \sim \va{\mu}$ then the distribution for $X_n \sim \va\mu(n)$ then what is the limit of $\va\mu(n)$ as $n\to \infty$. 
Suppose the limit $\nlim{n}{\infty}\va\mu P^n = \va\pi$ exists then we can write 
\begin{align*}
\va*{\pi} = \nlim{n}{\infty}\va*{\mu}\vb{P}^n = \nlim{n}{\infty}\va*{\mu}\vb{P}^{n-1}P = \nlim{n}{\infty} \va*{\mu}(n-1)\vb{P} = \va*{\pi}\vb{P}
\end{align*}
So $\va\pi$ is an \texttt{left eigenvector of $\vb{P}$} with \texttt{eigenvalue 1}. 

\begin{defn}[Stationary Distribution]
A probability vector $\va*{\pi}$ is the Stationary Distribution for the stochastic matrix $\vb{P}$ if 
\begin{align*}
&\nsum{k}{}\va*\pi_k = 1\\
&\va*{\pi}\vb{P} = \va*{\pi}
\end{align*}
\end{defn}
\begin{defn}[Stationary Measure]
A measure $\va\nu$ on $\mathcal{X}$ $\left(\va\nu \in \R^{\abs{\mathcal{X}}}\right)$ if 
\begin{align*}
&\va \nu_i \geq 0\\
&\nsum{}{}\va\nu_i > 0\\
&\va\nu \vb{P} = \va\nu
\end{align*}
\end{defn}
\begin{prop}[Stationary Distribution from Measure]
\label{statdm}
If $\abs{X} < \infty$ and $\va\nu$ is a stationary measure on $\vb P$  
\begin{align*}
\va*\pi = \frac{1}{\nsum{i}{}\va\nu_i}\va\nu
\end{align*}
Then $\va*\pi$ is a stationary distribution by definition.
\end{prop}
\begin{defn}[Bi-stochastic Matrix]
A \hyperref[stocmat]{stochastic matrix} is Bi-stochastic if 
\begin{align*}
&\nsum{\all(i)}{}P_{ij_0} = 1 & \text{ for fixed $j_0$}
\end{align*}
Sum of all rows = 1 and sum of all columns = 1.   
\end{defn}

\begin{prop}[Stationary Distribution for Bi-stochastic Matrices]
If $\vb P$ is a \textbf{Bi-stochastic} transition matrix for Markov chain with finite state space $\mathcal{X}$ with $\abs{\mathcal{X}} = N$ then the stationary distribution is given by 
\begin{align*}
\va*\pi = \mqty(\frac{1}{N} & \frac{1}{N} & \cdots & \frac{1}{N})
\end{align*}
\end{prop}
\newpage
\subsection{Detail Balance Condition}
\begin{defn}[Detail Balance Condition]
\label{detailb}
$\va\pi$ has the detail balance condition if:  
\begin{align*}
\va\pi_x \vb{P}_{xy} = \va \pi_y \vb{P}_{yx}
\end{align*}
\end{defn}
\begin{note}
Detail balance condition means $\P(X_1 = x, X_0 = y) = \P(X_1 = y , X_0 = x)$. 
\end{note}
\begin{thm}[Detail Balance and Stationary Distribution]
If $X_0 \sim \va\pi$ and $\va\pi$ satisfies the \hyperref[detailb]{detail balance condition} then $X_n \sim \va \pi$ for all $n\geq 1$
\end{thm}
\fig{fig1}{0.2}
\newpage
\section{Week 3}
\subsection{Communicating States}
\begin{defn}[communicating states]
A state $x$ communicates with $y$ if $\exists n\geq 1$ such that 
\begin{align*}
[\vb P^n]_{xy} > 0
\end{align*}
denoted by $x \to y$. 
\end{defn}
\begin{note}
$\P(A \given X_{n-1} = x) = \P_x(A)$ and $\E(\cdot \given X_n = x) = \E_x(\cdot)$
\end{note}
\begin{defn}[Time of the first return / first hitting time]
\[\tau_x = \min \{n \given X_n = x\}\]
\[\rho_{xy} = \P_x(\tau_y < \infty)\]
$\rho_{xy} = \P(\text{$X_n$ returns to $y$ given it starts at $x$)}$. 
\end{defn}
\begin{note}
\[1 - \rho_{xy} = \P_x(\tau_y = \infty)\]
\end{note}
\begin{lemma}[Communicating states and return probability]
$x\to y \iff \rho_{xy} > 0$.  
\end{lemma}
\begin{lemma}[Transitivity]
$x\to y$ and $y\to z \Rightarrow x \to z$
\end{lemma}
\begin{defn}[Time of $k-th$ return]
\label{kthret}
\begin{align*}
\tau_x^k = \min \{n > \tau_x^{k-1} \given X_n = x\}
\end{align*}
where $\tau_x^1 = \tau_x$. 
\end{defn}
\subsection{Recurrent and Transient States}
\begin{defn}[Recurrent and Transient States]
\label{recurrent}
A state $x\in \mathcal{X}$ is called \textbf{recurrent} if 
\begin{align*}
\rho_{xx} = 1
\end{align*}
and \textbf{transient} if 
\begin{align*}
\rho_{xx} < 1
\end{align*}
\end{defn}
\begin{thm}[Escaping path]
\label{escpath}
If $x\to y$ and $\rho_{xy} < 1$ then $x$ is transient.
\end{thm}
\begin{thm}[Corollary of {Escaping Path theorem}]
If $x\to y$ and $x$ is recurrent then $\rho_{xy} = 1$. 
\end{thm}
\newpage
\subsection{Strong Markov Property}
\begin{defn}[Stopping Time]
$T$ is a stopping time if the occurrence (or non occurrence) of the event $\{T = n\}$ can be determined by $\{X_0, \ldots, X_n\}$. 
\end{defn}
\begin{thm}[Strong Markov Property]
Suppose $T$ is a stopping time. Given $T = n$ and $X_T = y$ the random variables $\{X_{T+k}\}_{k=0}^\infty$ behave like a Markov chain starting from initial state $y$. That is 
\begin{align*}
\P(X_{T+1} = z \given X_T = y, T = n) = \P(X_1 = z\given X_0 = y) =\vb P_{yz}
\end{align*}
\end{thm}
\begin{lemma}[$k-$th return time and the strong Markov property]
Let $\tau_y^k$ be the \hyperref[kthret]{$k-$th return time to $y$}. Then the strong Markov property implies 
\begin{align*}
\P_x(\tau_y^k < \infty) = \rho_{xy}\rho_{yy}^{k-1}
\textbf{ or }
\P_y(\tau_y^k < \infty) = \rho_{yy}^{k} &&\forall k \geq 1
\end{align*}
\end{lemma}
\begin{note}
From the above \textbf{lemma} if we have $\rho_{yy} = 1$ ($y$ is recurrent) then the chain returns to $y$ for infinitely many $k$ and it continually
recurs in the Markov chain.\\
Otherwise if $\rho_{yy} < 1$ ($y$ is transient) then $\rho_{yy}^k \to 0$ as $k\to \infty$ so after sometime $y$ is never visited in the chain.  
\end{note}
\newpage
\section{Week 4}
\subsection{Classification of States}
\begin{defn}[Closed]
\label{closed}
A set $A$ is \textbf{closed} if  it is impossible to get out. Formally $c\in A$ and $y\notin A$ then $P_{xy} = 0$
\end{defn}
\begin{defn}[irreducible]
\label{irreducible}
A set $B$ is irreducible if every state is reachable from another in $k$ steps or every state communicates with with all other states. Formally 
\begin{align*}
x,y\in B \Rightarrow x \to y
\end{align*}
\end{defn}
\fig{fig2}{0.7}
\begin{lemma}[Commutating recurrent states]
If $x$ is \hyperref[recurrent]{recurrent} and $x\to y$ then $y$ is recurrent
\end{lemma}
\begin{lemma}[Existence of recurrent states in finite closed sets]
\label{finiteclosed}
If $A$ is finite and \hyperref[closed]{closed} then $\exists x \in A$ such that $x$ is \hyperref[recurrent]{recurrent}. 
\end{lemma}
\begin{thm}[Closed and irreducible sets are recurrent]
If $C\subseteq \mathcal{X}$ is \textbf{finite}, \hyperref[closed]{closed} and \hyperref[irreducible]{irreducible} then all $x\in C$ are \hyperref[recurrent]{recurrent}. 
\end{thm}
\begin{thm}[Decomposition Theorem]
If $\mathcal{X}$ is finite then 
\begin{align*}
\mathcal{X} = T \cup R_1 \cup R_2 \cup \cdots \cup R_k
\end{align*}
where $T$ is the set of transient states and $R_i$ for $1\leq i \leq k$ are are
closed irreducible sets of recurrent states. 
\end{thm}
\begin{defn}[Number of visits]
$N(y)$ is the number of visits to $y$ after initial time.  
\end{defn}
\begin{lemma}[Expected number of visits]
\begin{align*}
\E_x[N(y)] = \begin{cases}
0 & \rho_{xy} = 0\\
\frac{\rho_{xy}}{1-\rho_{yy}} & \rho_{xy} > 0
\end{cases}
\end{align*}
\end{lemma}
\newpage
\begin{lemma}[Expected number of visits II]
\begin{align*}
\E_x[N(y)] = \nsum{n=1}{\infty} [\vb P^n]_{xy}
\end{align*}
\end{lemma}

\begin{thm}[Equivalent condition for recurrence]
$y$ is recurrent if and only if 
\begin{align*}
\nsum{n=1}{\infty} [\vb P^n]_{yy} = \E_y[N(y)] = \infty
\end{align*}
\end{thm}
\subsection{Existence of Stationary measure}
\begin{thm}[Existence of Stationary measure]\label{thm411}
Suppose $\cal X$ is \hyperref[irreducible]{irreducible} and \hyperref[recurrent]{recurrent} there  exists a stationary measure $\va\mu$ with 
\begin{align*}
0<\mu_y < \infty && y\in \cal X
\end{align*}
Let $x\in \cal X$ be recurrent by \hyperref[finiteclosed]{Existence of recurrent states in finite closed sets}. We define $\va\mu^x$ as 
\begin{align*}
\mu_y^x = \E_x[\text{\# of visits to $y$ before $x$}] = \nsum{n=0}\infty \P_x(X_n = y, \tau_x > n)
\end{align*}
$\va\mu^x$ is a stationary measure for $\vb P$.
\end{thm}
\fig{fig3}{0.5}

\newpage
\section{Week 5}
\begin{defn}[Ergodicity]
If $\va\pi$ is a stationary distribution given $X_0 \sim \va\mu$  if we have 
\begin{align*}
\va\mu \vb P^n \rightarrow \va\pi
\end{align*}
then $\vb P$ has Ergodicity. 
\end{defn}
\begin{thm}[Ergodicity equivalent definition]
\begin{align*}
\va\mu \vb P^n \rightarrow \pi \iff [P^n]_{xy} \rightarrow \va\pi_y && \forall x\in \cal X
\end{align*}
\end{thm}
\begin{remark}
If $y$ is \hyperref[recurrent]{transient} then $\E_X[N(y)] < \infty$. Then 
\begin{align*}
\E_X[N(y)] = \nsum{n=1}{\infty} [\vb P^n]_{xy}
\end{align*}
Then $[\vb P^n]_{xy} \rightarrow 0$. Meaning $\va\pi_y = 0$, so all transient states have Ergodicity.
\end{remark}
\begin{defn}[Periodicity]
A state $x$ has a period $\dd_x$ if 
\begin{align*}
& I_x = \{n\geq 0 \given [P^n]_{xx} > 0\}
&\dd_x = \gcd I_x 
\end{align*}
$x$ is {\color{deepred} aperiodic} if $\dd_x = 1$ and {\color{deepred} periodic} if $\dd_x > 1$.   
\end{defn}

\begin{defn}[Class property]
A property $\cal K$ of a state $x$ is called a \textbf{class property} if 
$x$ has $\cal K$, $x\to y$ and $y\to x$ then $y$ has $\cal K$. 
\end{defn}

\begin{lemma}{Periodicity is a class property}
If $x\to y$ and $y\to x$ then $\dd_x = \dd_y$.
\end{lemma}

\begin{lemma}{$I_x$ is closed under addition}
\begin{align*}
a,b\in I_x \Rightarrow (a+b)\in I_x
\end{align*}
\end{lemma}

\begin{lemma}{}
If $x$ is aperiodic then $\exists n_0$ such that for all $n\geq n_0$ $n\in I_x$. 
\end{lemma}

\begin{lemma}{}
If $P_{xx} > 0$ then $x$ is aperiodic.
\end{lemma}
\newpage
\section{Week 6} 
\subsection{Convergence Theorems}
\begin{note}
\begin{align*}
&\mathbf{I}: \text{Irreducible}\\
&\mathbf{A}: \text{Aperiodic}\\
&\mathbf{R}: \text{Recurrent}\\
&\mathbf{S}: \text{Stationary distribution $\pi$ exists}
\end{align*}
\end{note}
\begin{remark}
$I, S \Rightarrow R$
\end{remark}
\begin{defn}[Convergence in Probability]
$\{X_n\}\pto X$ if $\forall \epsilon > 0$
\begin{align*}
\nlim{n}{\infty} \P(|X_n - X| > \epsilon) = 0
\end{align*}
\end{defn}
\begin{defn}[Almost surely convergence]
$\{X_n\}\asto X$ if 
\begin{align*}
\P\left(\nlim{n}{\infty}X_n = X\right) = 1
\end{align*}
\end{defn}
\begin{thm}[Weak Law of Large Numbers (WLLN)]
Let $\{X_k\}_{k\in \mathbb{N}} \sim \iid$ with $\E[X_k] = \mu$ then 
\begin{align*}
\frac{1}{n}\nsum{k=0}{n-1}X_k \pto \mu
\end{align*}
\end{thm}
\begin{thm}[Strong Law of Large Numbers (SLLN)]
Let $\{X_k\}_{k\in \mathbb{N}} \sim \iid$ with $\E[X_k] = \mu$ then 
\begin{align*}
\frac{1}{n}\nsum{k=0}{n-1}X_k \asto \mu
\end{align*}
\end{thm}
\begin{thm}[Convergence Theorem]\label{convthm}
If $I,A,S$ hold then 
\begin{align*}
P_{xy}^n \to \pi_y && n\to \infty
\end{align*}
\end{thm}
\begin{thm}[Asymptotic Frequency]\label{asymfreq}
Suppose $I,R$ hold and let $N_n(y)$ be the number of visits to $y$ upto time $n$ then 
\begin{align*}
\frac{N_n(y)}{n} \asto \frac{1}{\E_y \tau_y}
\end{align*} 
\end{thm}
\begin{proof}
If $R(k)$ is the $k-$th return time to $y$ then by SLLN $\frac{R(k)}{k} \asto \E_y[\tau_y]$ then we use squeeze theorem to get the result.   
\end{proof}
\newpage
\begin{lemma}[Recurrent States]
If $\mathbf{S}$ holds and $\pi_y > 0 \Rightarrow y$ is recurrent. 
\end{lemma}

\begin{thm}[Stationary Distribution Uniqueness]\label{uniquedist}
If $\mathbf{I}, \mathbf{S}$ hold then 
\begin{align*}
\pi_y = \frac{1}{\E_y[\tau_y]}
\end{align*}
\end{thm}

\begin{thm}[]\label{thm611}
Consider $\va\mu^x$ from this \hyperref[thm411]{theorem}
\begin{align*}
\va\mu^x_y = \nsum{n=0}{\infty} \P_x(X_n=y, \tau_x > n)
\end{align*}
If $\mathbf{I}, (\mathbf{R}), \mathbf{S}$ hold then 
\begin{align*}
\va\mu^x_y  = \frac{\va\pi_y}{\va\pi_x}
\end{align*} 
\end{thm}
\begin{proof}
$\nsum{y\in \mathcal{X}}{}\va\mu_y^x = \nsum{n=0}{\infty}\P_x(\tau_x > n) = \E_x[\tau_x] = \frac{1}{\pi_y}$
\end{proof}
\begin{thm}[Expected value of function]\label{expfun}
If $\mathbf{I}, \mathbf{S}$ hold and $\sum_x |f(x)|\va\pi_x < \infty$ then 
\begin{align*}
\frac{1}{n}\nsum{k=0}{n-1}f(X_k) \asto \nsum{x\in \mathcal{X}}{}f(x)\va\pi_x = \E_{\va\pi}[f(X)]
\end{align*}
\end{thm}
\begin{thm}[Stationary measures uniqueness]
If $\va\nu$ is a stationary measure then $\va\nu = \va\mu^x$ for some $x$. 
\end{thm}
\newpage
\section{Summary of Convergence Theorems}
\begin{tabular}{|c|c|c|}
\hline 
Name & Result & Conditions \\
\hline 
\hyperref[thm411]{Existence of stationary measure $\va\mu^x$} & 
$\va\mu^x_y = \nsum{n=0}{\infty}\P_x(X_n = y, \tau_x > n)$
 & $\mathbf{I}, \mathbf{R}$ \\ 
\hline 
\hyperref[convthm]{\textbf{Convergence Theorem}} & $[\vb P^n]_{xy} \to \va\pi_y$ as $n\to \infty$ & $\mathbf{I}, \mathbf{A}, \mathbf{S}$ \\
\hline 
\hyperref[asymfreq]{Asymptotic Frequency} & $\frac{N_n(y)}{n}\asto \frac{1}{\E_y[\tau_y]}$ & $\mathbf{I, R}$ \\ 
\hline 
\hyperref[uniquedist]{Stationary Distribution Uniqueness} & $\va\pi = \frac{1}{\E_y[\tau_y]}$ & $\mathbf{I,S}$ \\ 
\hline 
\hyperref[expfun]{Expected value of function} & 
$\frac{1}{n}\nsum{n=0}{\infty}\asto \nsum{x\in \mathcal{X}}{}f(x)\va\pi_x = \E_{\va\pi}[f(X)]$
 & $\mathbf{I, S}$, $\nsum{x\in \mathcal{X}}{}|f(x)|\va\pi_x < \infty$\\ 
\hline 
\hyperref[thm611]{Expected number of visits before $x$} & $\va\mu^x_y = \frac{\va\pi_y}{\va\pi_x}$ & $\mathbf{I, S}$ \\ 
\hline 
\end{tabular} 
\begin{note}
\begin{align*}
&\mathbf{I}: \text{Irreducible}\\
&\mathbf{A}: \text{Aperiodic}\\
&\mathbf{R}: \text{Recurrent}\\
&\mathbf{S}: \text{Stationary distribution $\pi$ exists}
\end{align*}
\end{note}
\begin{remark}
$I, S \Rightarrow R$
\end{remark}
\newpage
\section{Week 7}
\subsection{Exit Distributions}
\begin{defn}[Visiting Time for Set]
\begin{align*}
V_A = \min \{n\geq 1 : X_n \in A\}
\end{align*}
\end{defn}
\begin{thm}[Exit Distribution]
For a Markov chain on state space $\cal X$. Let $A, B \subseteq \cal X$ and let $C = {\cal X}\setminus (A\cup B)$ if we have 
\begin{align*}
&h(a) = 1 & a\in A\\
&h(b) = 0 & b\in B\\
&h(x) = \nsum{y\in {\cal X}}{}[\vb P]_{xy}h(y) & x\in C
\end{align*}
If $\P_x(V_A \wedge V_B < \infty)$ for all $x\in C$ then $h(x) = P_x(V_A < V_B)$ (Chain visits $A$ before $B$). 
\end{thm}

\subsection{Exit Times}
\begin{thm}[Exit Time]
Let $T = \min\{n\geq 0 : X_n \in A\}$ be the time to exit. Suppose $C = {\cal X}\setminus A$ is finite and $\P_x(T < \infty)>0$ for all $x\in C$. Then we define 
\begin{align*}
&g(a) = 0 & a\in A\\
&g(x) = 1 + \nsum{y\in {\cal X}}{}P_{xy}g(y)
\end{align*}
Then $g(x) = \E_x[T]$
\end{thm}
\subsection{Laplace distribution}
\begin{defn}[Laplace Matrix]
The matrix $L = P - I$ is called the the Laplace matrix
\end{defn}
For the exit distribution $h(x) = \P_x(V_A < V_B)$ it satisfies the Laplace equation: 
\begin{align*}
\begin{cases}
Lh = 0 & \text{on $C = {\cal X}\setminus (A\cup B)$}\\
h = 1 & \text{on $A$}\\
h = 0 & \text{on $B$}
\end{cases}
\end{align*}
For the exit time $g(x) = \E_x[T]$ it solves the \textit{Poisson equation}
\begin{align*}
\begin{cases}
Lg = -1 & \text{on $C$}\\
g = 0 & \text{on ${\cal X}\setminus C$}
\end{cases}
\end{align*}

























\end{document}